{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea3e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "# Ensure project root (the folder containing 'scripts') is on sys.path\n",
    "p = Path.cwd()\n",
    "for _ in range(10):\n",
    "    if (p / 'scripts').exists():\n",
    "        sys.path.insert(0, str(p))\n",
    "        break\n",
    "    if p.parent == p:\n",
    "        break\n",
    "    p = p.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f7bb3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parse_crosswalk_csv() missing 1 required positional argument: 'file_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuild_crosswalk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbuild_crosswalk\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Example: parse_crosswalk_csv with raw string path\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mbuild_crosswalk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse_crosswalk_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: parse_crosswalk_csv() missing 1 required positional argument: 'file_path'"
     ]
    }
   ],
   "source": [
    "import scripts.build_crosswalk as build_crosswalk\n",
    "# Example: parse_crosswalk_csv with raw string path\n",
    "build_crosswalk.parse_crosswalk_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb6a5f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1008676116.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mupdate build_icd10_reference.py to read .txt file based on below metatadata from  data\\raw_umls\\icd10cm-codes-April-1-2026.txt\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "update extract_snomed_diabetes.py to read .txt file based on below metatadata from   SnomedCT_ManagedServiceUS_PRODUCTION_US1000124_20250901T120000Z->SnomedCT_ManagedServiceUS_PRODUCTION_US1000124_20250901T120000Z->Snapshot->Terminology\n",
    "id\teffectiveTime\tactive\tmoduleId\tconceptId\tlanguageCode\ttypeId\tterm\tcaseSignificanceId\n",
    "101013\t20170731\t1\t900000000000207008\t126813005\ten\t900000000000013009\tNeoplasm of anterior aspect of epiglottis\t900000000000448009\n",
    "102018\t20170731\t1\t900000000000207008\t126814004\ten\t900000000000013009\tNeoplasm of junctional region of epiglottis\t900000000000448009\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26bc11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Download raw data files (from CDC/UMLS): DONE\n",
    "2. Run the data preparation scripts: DONE\n",
    "3. Precompute embeddings:\n",
    "python scripts/precompute_embeddings.py: Done (stored in data\\reference\\snomed_embeddings\\biobert.npz)\n",
    "4. Install Ollama (for Phase 4 LLM validation): Installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "229c9583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\ClinicalNLP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 438kB [00:00, 1.67MB/s]                    \n",
      "2026-02-17 20:11:34 INFO: Downloaded file to C:\\Users\\wasim_xhy2aoh\\stanza_resources\\resources.json\n",
      "2026-02-17 20:11:34 INFO: Downloading these customized packages for language: en (English)...\n",
      "====================================\n",
      "| Processor       | Package        |\n",
      "------------------------------------\n",
      "| tokenize        | mimic          |\n",
      "| pos             | mimic_charlm   |\n",
      "| lemma           | mimic_nocharlm |\n",
      "| depparse        | mimic_charlm   |\n",
      "| ner             | i2b2           |\n",
      "| backward_charlm | mimic          |\n",
      "| pretrain        | mimic          |\n",
      "| forward_charlm  | mimic          |\n",
      "====================================\n",
      "\n",
      "2026-02-17 20:11:34 INFO: File exists: C:\\Users\\wasim_xhy2aoh\\stanza_resources\\en\\tokenize\\mimic.pt\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.11.0/models/pos/mimic_charlm.pt: 100%|██████████| 39.2M/39.2M [00:01<00:00, 31.8MB/s]\n",
      "2026-02-17 20:11:38 INFO: Downloaded file to C:\\Users\\wasim_xhy2aoh\\stanza_resources\\en\\pos\\mimic_charlm.pt\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.11.0/models/lemma/mimic_nocharlm.pt: 100%|██████████| 4.19M/4.19M [00:00<00:00, 33.6MB/s]\n",
      "2026-02-17 20:11:40 INFO: Downloaded file to C:\\Users\\wasim_xhy2aoh\\stanza_resources\\en\\lemma\\mimic_nocharlm.pt\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.11.0/models/depparse/mimic_charlm.pt: 100%|██████████| 146M/146M [00:03<00:00, 36.5MB/s] \n",
      "2026-02-17 20:11:47 INFO: Downloaded file to C:\\Users\\wasim_xhy2aoh\\stanza_resources\\en\\depparse\\mimic_charlm.pt\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.11.0/models/ner/i2b2.pt: 100%|██████████| 49.8M/49.8M [00:02<00:00, 20.5MB/s]\n",
      "2026-02-17 20:11:51 INFO: Downloaded file to C:\\Users\\wasim_xhy2aoh\\stanza_resources\\en\\ner\\i2b2.pt\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.11.0/models/backward_charlm/mimic.pt: 100%|██████████| 18.9M/18.9M [00:00<00:00, 19.8MB/s]\n",
      "2026-02-17 20:11:54 INFO: Downloaded file to C:\\Users\\wasim_xhy2aoh\\stanza_resources\\en\\backward_charlm\\mimic.pt\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.11.0/models/pretrain/mimic.pt: 100%|██████████| 82.7M/82.7M [00:02<00:00, 34.8MB/s]\n",
      "2026-02-17 20:11:59 INFO: Downloaded file to C:\\Users\\wasim_xhy2aoh\\stanza_resources\\en\\pretrain\\mimic.pt\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.11.0/models/forward_charlm/mimic.pt: 100%|██████████| 18.9M/18.9M [00:00<00:00, 29.1MB/s]\n",
      "2026-02-17 20:12:02 INFO: Downloaded file to C:\\Users\\wasim_xhy2aoh\\stanza_resources\\en\\forward_charlm\\mimic.pt\n",
      "2026-02-17 20:12:02 INFO: Finished downloading models and saved to C:\\Users\\wasim_xhy2aoh\\stanza_resources\n",
      "2026-02-17 20:12:02 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 438kB [00:00, 1.67MB/s]                    \n",
      "2026-02-17 20:12:03 INFO: Downloaded file to C:\\Users\\wasim_xhy2aoh\\stanza_resources\\resources.json\n",
      "2026-02-17 20:12:04 INFO: Loading these models for language: en (English):\n",
      "==============================\n",
      "| Processor | Package        |\n",
      "------------------------------\n",
      "| tokenize  | mimic          |\n",
      "| pos       | mimic_charlm   |\n",
      "| lemma     | mimic_nocharlm |\n",
      "| depparse  | mimic_charlm   |\n",
      "| ner       | i2b2           |\n",
      "==============================\n",
      "\n",
      "2026-02-17 20:12:04 INFO: Using device: cpu\n",
      "2026-02-17 20:12:04 INFO: Loading: tokenize\n",
      "2026-02-17 20:12:06 INFO: Loading: pos\n",
      "2026-02-17 20:12:07 INFO: Loading: lemma\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'charlm_forward_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# download and initialize a mimic pipeline with an i2b2 NER model\u001b[39;00m\n\u001b[32m      3\u001b[39m stanza.download(\u001b[33m'\u001b[39m\u001b[33men\u001b[39m\u001b[33m'\u001b[39m, package=\u001b[33m'\u001b[39m\u001b[33mmimic\u001b[39m\u001b[33m'\u001b[39m, processors={\u001b[33m'\u001b[39m\u001b[33mner\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mi2b2\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m nlp = \u001b[43mstanza\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmimic\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mner\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mi2b2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# annotate clinical text\u001b[39;00m\n\u001b[32m      6\u001b[39m doc = nlp(\u001b[33m'\u001b[39m\u001b[33mThe patient had a sore throat and was treated with Cepacol lozenges.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ClinicalNLP\\.venv\\Lib\\site-packages\\stanza\\pipeline\\core.py:308\u001b[39m, in \u001b[36mPipeline.__init__\u001b[39m\u001b[34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, resources_filepath, proxies, foundation_cache, device, allow_unknown_language, **kwargs)\u001b[39m\n\u001b[32m    305\u001b[39m logger.debug(curr_processor_config)\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    307\u001b[39m     \u001b[38;5;66;03m# try to build processor, throw an exception if there is a requirements issue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     \u001b[38;5;28mself\u001b[39m.processors[processor_name] = \u001b[43mNAME_TO_PROCESSOR_CLASS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprocessor_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurr_processor_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m                                                                              \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m                                                                              \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessorRequirementsException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    312\u001b[39m     \u001b[38;5;66;03m# if there was a requirements issue, add it to list which will be printed at end\u001b[39;00m\n\u001b[32m    313\u001b[39m     pipeline_reqs_exceptions.append(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ClinicalNLP\\.venv\\Lib\\site-packages\\stanza\\pipeline\\lemma_processor.py:32\u001b[39m, in \u001b[36mLemmaProcessor.__init__\u001b[39m\u001b[34m(self, config, pipeline, device)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mself\u001b[39m._use_identity = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mself\u001b[39m._pretagged = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ClinicalNLP\\.venv\\Lib\\site-packages\\stanza\\pipeline\\processor.py:193\u001b[39m, in \u001b[36mUDProcessor.__init__\u001b[39m\u001b[34m(self, config, pipeline, device)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28mself\u001b[39m._vocab = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_variant\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_up_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# build the final config for the processor\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[38;5;28mself\u001b[39m._set_up_final_config(config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ClinicalNLP\\.venv\\Lib\\site-packages\\stanza\\pipeline\\lemma_processor.py:58\u001b[39m, in \u001b[36mLemmaProcessor._set_up_model\u001b[39m\u001b[34m(self, config, pipeline, device)\u001b[39m\n\u001b[32m     56\u001b[39m lemma_classifier_args = \u001b[38;5;28mdict\u001b[39m(args)\n\u001b[32m     57\u001b[39m lemma_classifier_args[\u001b[33m'\u001b[39m\u001b[33mwordvec_pretrain_file\u001b[39m\u001b[33m'\u001b[39m] = config.get(\u001b[33m'\u001b[39m\u001b[33mpretrain_path\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28mself\u001b[39m._trainer = \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_path\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfoundation_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfoundation_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemma_classifier_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlemma_classifier_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ClinicalNLP\\.venv\\Lib\\site-packages\\stanza\\models\\lemma\\trainer.py:38\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, args, vocab, emb_matrix, model_file, device, foundation_cache, lemma_classifier_args)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args=\u001b[38;5;28;01mNone\u001b[39;00m, vocab=\u001b[38;5;28;01mNone\u001b[39;00m, emb_matrix=\u001b[38;5;28;01mNone\u001b[39;00m, model_file=\u001b[38;5;28;01mNone\u001b[39;00m, device=\u001b[38;5;28;01mNone\u001b[39;00m, foundation_cache=\u001b[38;5;28;01mNone\u001b[39;00m, lemma_classifier_args=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m         \u001b[38;5;66;03m# load everything from file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfoundation_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemma_classifier_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     40\u001b[39m         \u001b[38;5;66;03m# build model from scratch\u001b[39;00m\n\u001b[32m     41\u001b[39m         \u001b[38;5;28mself\u001b[39m.args = args\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\ClinicalNLP\\.venv\\Lib\\site-packages\\stanza\\models\\lemma\\trainer.py:303\u001b[39m, in \u001b[36mTrainer.load\u001b[39m\u001b[34m(self, filename, args, foundation_cache, lemma_classifier_args)\u001b[39m\n\u001b[32m    301\u001b[39m \u001b[38;5;28mself\u001b[39m.args = checkpoint[\u001b[33m'\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28mself\u001b[39m.args[\u001b[33m'\u001b[39m\u001b[33mcharlm_forward_file\u001b[39m\u001b[33m'\u001b[39m] = args.get(\u001b[33m'\u001b[39m\u001b[33mcharlm_forward_file\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcharlm_forward_file\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m    304\u001b[39m     \u001b[38;5;28mself\u001b[39m.args[\u001b[33m'\u001b[39m\u001b[33mcharlm_backward_file\u001b[39m\u001b[33m'\u001b[39m] = args.get(\u001b[33m'\u001b[39m\u001b[33mcharlm_backward_file\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mself\u001b[39m.args[\u001b[33m'\u001b[39m\u001b[33mcharlm_backward_file\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    305\u001b[39m \u001b[38;5;28mself\u001b[39m.word_dict, \u001b[38;5;28mself\u001b[39m.composite_dict = checkpoint[\u001b[33m'\u001b[39m\u001b[33mdicts\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mKeyError\u001b[39m: 'charlm_forward_file'"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "# download and initialize a mimic pipeline with an i2b2 NER model\n",
    "stanza.download('en', package='mimic', processors={'ner': 'i2b2'})\n",
    "nlp = stanza.Pipeline('en', package='mimic', processors={'ner': 'i2b2'})\n",
    "# annotate clinical text\n",
    "doc = nlp('The patient had a sore throat and was treated with Cepacol lozenges.')\n",
    "# print out all entities\n",
    "for ent in doc.entities:\n",
    "    print(f'{ent.text}\\t{ent.type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff01f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
